{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from preprocess_dataset import load_dataset\n",
    "from preprocess_dataset import cut_out_zero_rows\n",
    "from preprocess_dataset import cut_out_zero_cols\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import errno\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mkdir(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as exception:\n",
    "        if exception.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "def compute_stats(data):\n",
    "    tmp = np.count_nonzero(data, axis=1)\n",
    "    print(\"Average/Min/Max non-zero count for each cell: \", np.mean(tmp), np.min(tmp), np.max(tmp))\n",
    "    tmp = np.count_nonzero(data, axis=0)\n",
    "    print(\"Average/Min/Max non-zero count for each gene: \", np.mean(tmp), np.min(tmp), np.max(tmp))\n",
    "    if np.max(tmp) == data.shape[0]: print(\"there exists a gene active in all cells!\")\n",
    "    tmp = np.max(data, axis=0)\n",
    "    print(\"Average/Min/Max max-expression value for each gene: \", np.mean(tmp), np.min(tmp), np.max(tmp))\n",
    "    print(\"Number of genes with max-expression of 1:\", np.where(tmp==1)[0].shape[0])\n",
    "    # mean non-zero expression value of each gene\n",
    "    tmp = np.true_divide(data.sum(0),(data!=0).sum(0))\n",
    "    print(\"Average/Min/Max mean non-zero expression value for each gene: \", np.mean(tmp), np.min(tmp), np.max(tmp))\n",
    "    \n",
    "def normalize_along_columns(data):\n",
    "    return (data - data.mean(axis=0)) / data.std(axis=0)\n",
    "\n",
    "def split_test(data, test_size=0.1):\n",
    "    data_trn,data_te,_,_ = train_test_split(data, np.ones(data.shape[0]), test_size=test_size, random_state=42)\n",
    "    return data_trn,data_te\n",
    "\n",
    "def gene_dropout(data_trn,data_te,dropout=0.1):\n",
    "    column_dropout = 0.1\n",
    "    rand_idx = np.random.randint(data_trn.shape[1], size=int(data_trn.shape[1]*column_dropout))\n",
    "    trimmed_data_trn = data_trn[:,rand_idx]\n",
    "    trimmed_data_te = data_te[:,rand_idx]\n",
    "    \n",
    "    trimmed_data_te = cut_out_zero_rows(pd.DataFrame(trimmed_data_te)).as_matrix()\n",
    "    keep_cols = ~(pd.DataFrame(trimmed_data_te) == 0).all(axis=0)\n",
    "    trimmed_data_te = cut_out_zero_cols(pd.DataFrame(trimmed_data_te)).as_matrix()\n",
    "    \n",
    "    return trimmed_data_trn, trimmed_data_te\n",
    "\n",
    "def save_model(model, folder = \"\", model_name=\"model\"):\n",
    "    folder = 'models/' + folder\n",
    "    mkdir(folder)\n",
    "    joblib.dump(model, folder + model_name + '.pickle')\n",
    "    joblib.dump(model, folder + model_name + '.pickle')\n",
    "\n",
    "def save_data(data, genes, path, as_txt=False):\n",
    "    folder = \"lda_data_bak/\"\n",
    "    mkdir(folder)\n",
    "    np.save(folder + path + \".npy\", data)\n",
    "    if as_txt:\n",
    "        with open(folder + path + \".txt\", 'w') as f:\n",
    "            for row in data:\n",
    "                idxs = np.nonzero(row)\n",
    "                f.write(\" \".join([\"%s %d\" % t for t in zip(genes[idxs], row[idxs])]))\n",
    "                f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/test data shape: (4180, 20441) (465, 20441)\n",
      "# top words: 15\n"
     ]
    }
   ],
   "source": [
    "data, genes, cells = load_dataset(filter_std=0.1)\n",
    "data_tr, data_te = split_test(data)\n",
    "save_data(data_tr, genes, \"train_data\", as_txt=True)\n",
    "save_data(data_te, genes, \"test_data\", as_txt=True)\n",
    "print(\"train/test data shape:\", data_tr.shape, data_te.shape)\n",
    "\n",
    "run_ovb = True; run_gibbs = True\n",
    "n_top_words = 15\n",
    "print(\"# top words: %d\" % n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sklearn online variational bayes (with mini-batches) \n",
    "(hoffman, blei, et al. https://pdfs.semanticscholar.org/157a/ef34d39c85d6576028f29df1ea4c6480a979.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_genes = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print((\"topic %d:\" % topic_idx) + \", \".join(top_genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if run_ovb:\n",
    "    print(\"online variational bayes with sklearn\")\n",
    "    for n_clusters in [5,10,25,50]:\n",
    "        t0 = time.time()\n",
    "        print(\"number of clusters:\", n_clusters)\n",
    "        lda_trainer = LatentDirichletAllocation(n_clusters, n_jobs = 2, random_state=17)\n",
    "        lda_trained = lda_trainer.fit(data_tr)\n",
    "        print(\"\\ttraining time:\",time.time()-t0)\n",
    "        save_model(lda_trained, folder='online_vb/', model_name=str(n_clusters))\n",
    "        print_top_words(lda_trained, genes, n_top_words)\n",
    "        log_likelihood = lda_trained.score(data_te)\n",
    "        perplexity = lda_trained.perplexity(data_te)\n",
    "        print(\"\\tlog likelihood:\", log_likelihood)\n",
    "        print(\"\\tperplexity:\", perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### parallelized gibbs sampling\n",
    "(liu, et al.https://github.com/openbigdatagroup/plda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_top_words(topic_dists, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(topic_dists):\n",
    "        top_genes = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        print((\"topic %d:\" % topic_idx) + \", \".join(top_genes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collapsed gibbs sampling with lda\n",
      "number of clusters: 5\n",
      "executing mpiexec -n 2 ./plda/mpi_lda --num_topics 5 --alpha 0.1 --beta 0.01 --training_data_file lda_data_bak/train_data.txt --model_file results/gibbs/5_model.txt --burn_in_iterations 100 --total_iterations 100\n",
      "\ttraining time: 387.3934805393219\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'print_top_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-ea6b2162129b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtopic_dists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_frm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mgenes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_frm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mprint_top_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_dists\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_top_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'print_top_words' is not defined"
     ]
    }
   ],
   "source": [
    "if run_gibbs:\n",
    "    print(\"collapsed gibbs sampling with lda\")\n",
    "\n",
    "    cmd1 = \"mpiexec -n 2 ./plda/mpi_lda --num_topics %d --alpha 0.1 --beta 0.01 --training_data_file %s \"\n",
    "    cmd2 = \"--model_file %s --burn_in_iterations 100 --total_iterations 100\"\n",
    "    cmd = cmd1 + cmd2\n",
    "\n",
    "    mkdir(\"results/gibbs/\")\n",
    "\n",
    "    for n_clusters in [5,10,25,50]:\n",
    "        print(\"number of clusters:\", n_clusters)\n",
    "        final_model_path = \"results/gibbs/%d_model.txt\" % n_clusters\n",
    "        final_cmd = cmd % (n_clusters, \"lda_data_bak/train_data.txt\", final_model_path)\n",
    "\n",
    "        print(\"executing\", final_cmd)\n",
    "        t0 = time.time()\n",
    "        os.system(final_cmd)\n",
    "        print(\"\\ttraining time:\",time.time()-t0)\n",
    "\n",
    "        model_frm = pd.read_csv(final_model_path, sep=r\"\\s+\", header = None, index_col = 0)\n",
    "        topic_dists = model_frm.as_matrix().T\n",
    "        genes = list(model_frm.index)\n",
    "        print_top_words(topic_dists, genes, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
