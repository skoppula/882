%!TEX program = xelatex

\documentclass[11pt]{article}

\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{titlesec}
\usepackage{wrapfig}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{sidecap}
\usepackage{clrscode}
\usepackage{xcolor}
\usepackage{float}
\usepackage{stackengine}

\usepackage[margin=0.75in]{geometry}

\newlength{\tindent}
\setlength{\tindent}{\parindent}
\setlength{\parindent}{0pt}
\renewcommand{\indent}{\hspace*{\tindent}}
\floatplacement{figure}{H}

\newcommand\tab[1][1cm]{\hspace*{#1}}
\newcommand{\SK}{\textbf{[SK]}\tab}
\newcommand{\KY}{\textbf{[KY]}\tab}
\newcommand{\CO}{\textbf{[CO]}\tab}
\newcommand\cdate[1][1cm]{\textbf{[#1]}}

\renewcommand{\thesubsection}{\thesection.\alph{subsection}}
\titleformat{\subsection}[runin]{\normalfont\large\bfseries}{\thesubsection}{1em}{}

\begin{document}

\begin{centering}
\Large
    \textbf{Bayesian Clustering and Topic Modeling of Gene Expression Data}  \\
    \vspace{2mm}
    \normalsize
    Skanda Koppula (\url{skoppula@mit.edu}), Karren Yang (\url{karren@mit.edu}) \\
    \vspace{2mm}
    \normalsize
    6.882 Project Proposal, \today \\
\end{centering}
\vspace{5mm}

\section{Overview}

We explored the usage of various topic and clustering models in the analysis of gene expression data. For the former, we explored whether topic modeling could identify biologically relevant `topics'. In this context, a `topic' is a set of genes that together perform a specific biological function (a `gene module'); we compared our results to modules found by biology literate. For the latter, we explored whether we are able to cluster cells
accurately based on their gene expression levels. \footnote{The genes in gene modules generally move in tandem: so when a gene module is upregulated, all genes have higher expression (i.e. frequency). This motivates our use of topic models to capture this relationship.}.

\section{Topic Model: Latent Dirichlet Allocation}

We were unable to yield any results using Python's out-of-the-box implementation of \texttt{lda} \cite{lda}. With our 250 MB dataset, the collapsed Gibbs sampler used in the implementation was taking too long to produce samples, even on larger server-grade machines and with a small number of LDA clusters. We explored modifying the source to parallelize the sampling, but found the source to be crabbed and hard to follow. \\

In our search for an alternative, we explored another implementation that used online mean-field variational bayes for posterior estimate \cite{ovb, online}, and a broken \texttt{C++} Gibbs sampler for LDA that we modified to work \cite{plda}. We were able to get this latter sampler running parallel across multiple cores, with a burn-in of 100 iterations. We compare these two posterior estimation approaches using our entire dataset, with a 10\% held-out testing partition. We experimented with
$k=5,10,25,\text{ and } 50$ clusters. \\

Figure \ref{fig:time} in the Appendix shows the time to complete each estimation method. As expected, Gibbs scales poorly with the parameter dimensionality and is strictly worse than online variational Bayes across all studied topic counts. \\

We had time to calculate perplexity and log-likelihood for the OVB parameters, and very surprisingly, we find an increasing log-likelihood for larger cluster sizes on a held-out test set (Figure \ref{fig:ll}). We are currently trying to understand whether this is because of programming error or a legitimate result. \\

As is standard in computational gene module method validation, we use a hypergeometric test to compare our topics' collections of genes, with existing collections of genes catalogued in the comprehensive gene module database, \texttt{MSigDB} \cite{msigdb}. Figure \ref{fig:pathways} in the Appendix shows topic-to-\texttt{MSigDB} module matches for which the $p$-value of the match is at least less than 0.3. Models with larger clusters tend to have more matches with higher significance. There
a very slightly higher number of matches using the topics discovered by parallelized Gibbs. There are repeat matches, which is slightly disheartening. We note however that these $p$-values do \textit{not} factor for multiple hypothesis corrections (yet), so could be misleadingly significant!

\section{Clustering Model: Finite-Sized Mixture Model}

We are currently implementing from scratch the mixture model specified by the plate model in Figure \ref{fig:plate}. The underlying assumption of behind the model is that the cell cluster assignment, $z_i$, would be indicative of it's gene expression values as well. Our implementation uses \texttt{numpy}, and we are exploring backends to make posterior inference computationally feasible (our current implementation, which uses a Gibbs sampler, overflows memory, so we are 
implementing stochastic variational methods, like used in our LDA experiments).

\section{Remaining Work and Schedule}
We are making progress as per the schedule outlined in our proposal. Apart from continuing with work on the finite-sized mixture model, we plan on running a few experiments to test robustness of our methods to dropout, and scaling IBP and HDP to the full dataset.

\section{Appendix}
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{time}
    \caption{Comparison of the running times of each of the posterior estimation methods across various cluster sizes.}
    \label{fig:time}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{ll}
    \caption{Log-likelihood of the held-out testing set, across various cluster sizes.}
    \label{fig:ll}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{pathways}
    \caption{Matches between the gene collections found in LDA topics and published gene sets in \texttt{MSigDB}.}
    \label{fig:pathways}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.3\textwidth]{mixture}
    \caption{Finite \texttt{K}-sized mixture model currently implemented. $\theta$ is the parameter for every cluster component, represented from a categorical draw of over all genes. $z_i$ is the cluster assignment, and $\phi$ is the distribution of clusters. As usual, $\alpha, \beta$ are hyper-parameters.}
    \label{fig:plate}
\end{figure}

\begin{thebibliography}{9}
    \bibitem{hg} 
        The XL-mHG Test For Enrichment: A Technical Report.
        \url{https://arxiv.org/pdf/1507.07905.pdf}
         
    \bibitem{msigdb} 
        Molecular Signatures Database v6.0.
        \url{http://software.broadinstitute.org/gsea/msigdb}
         
    \bibitem{lda} 
        lda: Topic modeling with latent Dirichlet Allocation.
        \url{http://pythonhosted.org/lda/}

    \bibitem{ovb} 
        Online Latent Dirichlet Allocation with variational inference.
        \url{https://github.com/scikit-learn/scikit-learn/blob/master/sklearn/decomposition/online_lda.py}

    \bibitem{plda} 
        C++ implementation of Latent Dirichlet Allocation
        \url{https://github.com/openbigdatagroup/plda/blob/master/lda.cc}

    \bibitem{online}
        Online Learning for Latent Dirichlet Allocation.
        \url{https://pdfs.semanticscholar.org/157a/ef34d39c85d6576028f29df1ea4c6480a979.pdf}

    \bibitem{HDP}
        Hierarchical Dirichlet Processes
        \url{http://people.eecs.berkeley.edu/~jordan/papers/hdp.pdf}

    \bibitem{IBP}
        The IBP Compound Dirichlet Process and its Application to Focused Topic Modeling
        \url{http://www.cs.columbia.edu/~blei/papers/WilliamsonWangHellerBlei2010.pdf}

    \bibitem{TS-DDP}
        A Time-Series DDP for Functional Proteomics Profiles
        \url{https://www.ma.utexas.edu/users/pmueller/pap/NM12.pdf}


\end{thebibliography}


\end{document}
